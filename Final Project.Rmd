---
title: "FinalProject"
author: "Cole Maxwell"
date: "4/18/2022"
output:
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Wine Data Machine Learning Models

These data were obtained from the University of California Irvine's Machine [Learning Repository](https://archive.ics.uci.edu/ml/datasets/wine). These data are the results of a chemical analysis of wines grown in the same region in Italy but derived from three different cultivars. These models attempt to predict they `Type` of wine (White or Red) and determine the `quality` of both white and red wines.

## Data Pre-Processing 

```{r, style="display: flex;"}
library(knitr)
library(keras)
library(tensorflow)
library(ggplot2)


#Loading wine csv files
WhiteWineQualityDataFrame <- read.csv("winequality-white.csv", sep = ";")
RedWineQualityDataFrame <- read.csv("winequality-red.csv", sep = ";")
WineQualityDataFrame <- rbind(WhiteWineQualityDataFrame, RedWineQualityDataFrame)

# Various syntax for R dataframes

summary(WineQualityDataFrame)
head(WineQualityDataFrame) %>% kable()

```

\clearpage

## Exploritory Analysis 
```{r}
# Evaluation of the correlation variables
WineQualityMatrix <- data.matrix(WineQualityDataFrame)
WineQualityCorrelation <- cor(WineQualityMatrix)

# Generate a heatmap of correlations
heatmap(WineQualityCorrelation, 
        margins = c(8, 8), 
        main = "Vaiable Correspondence with Quality",
        )
```

\clearpage

```{r}
table(WineQualityDataFrame$quality) %>% 
  kable(col.names = c("Quality Level", "Frequency"),
        )

hist(WineQualityDataFrame$quality, 
     xlab = "Quality",
     main = "Frequency of Each Quality Catagory"
     )
```
## Dealing With Imbalanced Classification

Machine Learning algorithms tend struggle when faced with imbalanced classification data sets. Sampling Methods can be used to modify an imbalanced data into balanced distribution using several different mechanisms

Methods Include:

1. Undersampling
2. Oversampling
3. Synthetic Data Generation
4. Cost Sensitive Learning

The ROSE (Random Over Sampling Examples) package helps us to generate artificial data based on sampling methods and smoothed bootstrap approach. This package has well defined accuracy functions to do the tasks quickly.



```{r, eval=F}
library(ROSE)

WineQualityDataFrame$quality <- as.factor(WineQualityDataFrame$quality)

data_balanced_both <- ovun.sample(quality ~ ., data = WineQualityDataFrame, method = "both", p=0.5, N=2000, seed = 1)$data

```


\clearpage

### Splitting data into training and testing

```{r}
# Get 80% of the rows from the data set 
sample_size <- round(nrow(WineQualityDataFrame) * 0.8)

# setting random seed to make results repeatable
set.seed(1234) 


# Subtract three from the quality column so quality levels start at zero
WineQualityDataFrame$quality <- WineQualityDataFrame$quality -3

# Separate data into training and testing sets
picked <- sample(seq_len(nrow(WineQualityDataFrame)),size = sample_size)
training <- WineQualityDataFrame[picked,]
testing <- WineQualityDataFrame[-picked,]


# Changing y into categorical data (performing one-hot encoding)
yTr <- to_categorical(training$quality, num_classes = 7)
yTest <- to_categorical(testing$quality, num_classes = 7)

```

## Neural network for Wine Quality

This is where we tried to improve our accuracy. One way we found that was somewhat successful
was to increase the number of epochs. We think Elena chose 20 epochs because she had a smaller data
set but since ours was larger we needed more. We found that after 300 we got vastly diminishing
returns, so we settled on 300 for the time being. We are still looking at ways of improving accuracy.

initial accuracy -> ~43%
drop Id feature -> no improvement in accuracy
increase epochs to 100 -> accuracy raised to ~53%
increase epochs to 1000 -> accuracy raised to ~67%
decrease epochs to 300 -> accuracy lowered to ~60%
increase units from 64 to 128 in first layer -> ~56%
increase input_shape from 4 to 11 -> ~80%
increase first layer units from 64 to 128 -> ~81%
increase second layer units from 64 to 128 -> ~88%


increased the sample_size from 120 to 500 -> increased 43% to 57% accuracy 
Removed two layers and had only one dropout layer at 25% -> accuracy: 0.5941
Removed two layers, removed dropout rate, now only look at 7 features, added regularization to second hidden layer -> 0.6277056
added third hidden layer, reduced first layer from 128 to 7 nodes, added regularization to three hidden layers -> 0.5844156
removed regularization from middle hidden layer -> 0.5454546
removed regularization from first hidden layer -> 0.5887446
add regularization to middle layer -> 0.5627705
7 hidden layer, layer 1 128 nodes, layers 2-7 64 nodes, regularization on layers 4 and 7 -> 0.5021645
add bias of 2.0 to first layer -> 0.5454546
3 hidden layers, regularization on last 2, bias of 2.0 on first layer -> 0.4935065
same as above but reg on hidden layers 1 and 3 -> 0.5324675
increase nodes in first layer back up from 7 to 128 -> 0.6190476
move bias from input to output node -> 0.5584416
add alcohol col as a parameter -> 100% (not the good 100 tho lol)
reduce nodes in input layer to 8 from 128 -> 100%
L1 = https://www.analyticssteps.com/blogs/l2-and-l1-regularization-machine-learning

Wow!!! Changed loss to binary_crossentropy -> accuracy: 0.8773
No increase in accuracy by changing epochs to 300

### Without Regularization
```{r, echo=FALSE , eval=FALSE}

wineModel = keras_model_sequential() %>%
  layer_dense(units = 32, activation = "relu",input_shape=(11)) %>%
  layer_dense(units = 32, activation = "relu") %>%
  layer_dense(units = 32, activation = "relu") %>%
  layer_dense(units = 32, activation = "relu")) %>%
  layer_dense(units = ncol(yTr), activation = "softmax")


wineModel %>% compile(
  loss = "categorical_crossentropy",
  optimizer = "adam", #optimizer_rmsprop(),
  metrics = "accuracy"
)

xTr <- as.matrix(training[,1:11]) # need to convert to a matrix
xTest <- as.matrix(testing[,1:11])

history <- wineModel %>% 
  fit(
    x = xTr, # input is the first 4 columns of the dataframe
    y = yTr, # label is the last column
    epochs = 50
  )

```

```{r, eval=FALSE}
plot(history)
summary(wineModel)
wineModel %>% evaluate(xTest, yTest)
```


### With l1 Regularization
```{r, echo=FALSE , eval=FALSE}

wineModel = keras_model_sequential() %>%
  layer_dense(units = 32, activation = "relu",input_shape=(11), regularizer_l1(l = 0.01)) %>%
  layer_dense(units = 32, activation = "relu", regularizer_l1(l = 0.01)) %>%
  layer_dense(units = 32, activation = "relu", regularizer_l1(l = 0.01)) %>%
  layer_dense(units = 32, activation = "relu", regularizer_l1(l = 0.01)) %>%
  layer_dense(units = ncol(yTr), activation = "softmax")


wineModel %>% compile(
  loss = "categorical_crossentropy",
  optimizer = "adam", #optimizer_rmsprop(),
  metrics = "accuracy"
)

xTr <- as.matrix(training[,1:11]) # need to convert to a matrix
xTest <- as.matrix(testing[,1:11])

history <- wineModel %>% 
  fit(
    x = xTr, # input is the first 4 columns of the dataframe
    y = yTr, # label is the last column
    epochs = 50
  )

```
```{r, eval=FALSE}
plot(history)
summary(wineModel)
```

# Evaluate the model
```{r, eval=FALSE}
wineModel %>% evaluate(xTest, yTest)

# Predicting likelihood of all categories:
result <- wineModel %>% predict(xTest)

result

testing[,11]

```

# Red and White Wine Classification

```{r}
#Loading a csv file
WhiteWineQualityDataFrame <- read.csv("winequality-white.csv", sep = ";")
WhiteWineQualityDataFrame["Type"] <- 0
RedWineQualityDataFrame <- read.csv("winequality-red.csv", sep = ";")
RedWineQualityDataFrame ["Type"] <- 1
WineQualityDataFrame <- rbind(WhiteWineQualityDataFrame, RedWineQualityDataFrame)

WineQualityDataFrame <- subset(WineQualityDataFrame, select = -c(quality))

#WineQualityDataFrame <- subset(WineQualityDataFrame, select = -c(quality))

```


# Exploritory Analysis

```{r}
table(WineQualityDataFrame$Type) %>% 
  kable(col.names = c("Type Level", "Frequency"),
        )
```

```{r}
library(ROSE)

WineQualityDataFrame$quality <- as.factor(WineQualityDataFrame$Type)

underSampledWine <- ovun.sample(Type ~ .,
                                data = WineQualityDataFrame,
                                method = "under",
                                N=3000,
                                seed = 1)$data


table(underSampledWine$Type) %>% 
  kable(col.names = c("Type Level", "Frequency"),
        )
```


```{r}

sample_size <- round(nrow(underSampledWine) * 0.8)
set.seed(1234) # setting random seed to make results repeatable

picked <- sample(seq_len(nrow(underSampledWine)),size = sample_size)
training <- underSampledWine[picked,]
testing <- underSampledWine[-picked,]


# Changing y into categorical data (performing one-hot encoding)

yTr <- to_categorical(training$Type, num_classes = 2)
yTest <- to_categorical(testing$Type, num_classes = 2)

```

# Model

```{r, echo=FALSE, eval=FALSE}

wineModel = keras_model_sequential() %>%
  layer_dense(units = 32, activation = "relu",input_shape=(11), regularizer_l1(l = 0.01)) %>%
  layer_dense(units = 32, activation = "relu", regularizer_l1(l = 0.01)) %>%
  layer_dense(units = 32, activation = "relu", regularizer_l1(l = 0.01)) %>%
  layer_dense(units = 32, activation = "relu", regularizer_l1(l = 0.01)) %>%
  layer_dense(units = ncol(yTr), activation = "softmax")


wineModel %>% compile(
  loss = "binary_crossentropy",
  optimizer = "adam", #optimizer_rmsprop(),
  metrics = "accuracy"
)

xTr <- as.matrix(training[,1:11]) # need to convert to a matrix
xTest <- as.matrix(testing[,1:11])

history <- wineModel %>% 
  fit(
    x = xTr, # input is the first 4 columns of the dataframe
    y = yTr, # label is the last column
    epochs = 50
  )

```

```{r, eval=FALSE}
plot(history)
summary(wineModel)
wineModel %>% evaluate(xTest, yTest)
```