---
title: "FinalProject"
author: "Cole Maxwell"
date: "4/18/2022"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r cars}
library(keras)
library(tensorflow)
library(tidyverse)


#Loading a csv file
WhiteWineQualityDataFrame <- read.csv("winequality-white.csv", sep = ";")
WhiteWineQualityDataFrame["Type"] <- 0
RedWineQualityDataFrame <- read.csv("winequality-red.csv", sep = ";")
RedWineQualityDataFrame ["Type"] <- 1
WineQualityDataFrame <- rbind(WhiteWineQualityDataFrame, RedWineQualityDataFrame)

WineQualityDataFrame <- subset(WineQualityDataFrame, select = -c(quality))

# Various syntax for R dataframes
summary(WineQualityDataFrame)

head(WineQualityDataFrame)

head(WineQualityDataFrame$quality)

```
```{r}
# Evaluation of the correlation variables
WineQualityCorrelation <- cor(WineQualityMatrix)
heatmap(WineQualityCorrelation, margins = c(10, 10))

```

## Splitting data into training and testing

```{r}
sample_size <- round(nrow(WineQualityDataFrame) * 0.8)
set.seed(1234) # setting random seed to make results repeatable


# Subtract three to so quality levels start at zero
WineQualityDataFrame$quality <- WineQualityDataFrame$quality -3

picked <- sample(seq_len(nrow(WineQualityDataFrame)),size = sample_size)
training <- WineQualityDataFrame[picked,]
testing <- WineQualityDataFrame[-picked,]


# Changing y into categorical data (performing one-hot encoding)

yTr <- to_categorical(training$Type, num_classes = 2)
yTest <- to_categorical(testing$Type, num_classes = 2)

```

## Neural network for Wine Quality

This is where we tried to improve our accuracy. One way we found that was somewhat successful
was to increase the number of epochs. We think Elena chose 20 epochs because she had a smaller data
set but since ours was larger we needed more. We found that after 300 we got vastly diminishing
returns, so we settled on 300 for the time being. We are still looking at ways of improving accuracy.

initial accuracy -> ~43%
drop Id feature -> no improvement in accuracy
increase epochs to 100 -> accuracy raised to ~53%
increase epochs to 1000 -> accuracy raised to ~67%
decrease epochs to 300 -> accuracy lowered to ~60%
increase units from 64 to 128 in first layer -> ~56%
increase input_shape from 4 to 11 -> ~80%
increase first layer units from 64 to 128 -> ~81%
increase second layer units from 64 to 128 -> ~88%


increased the sample_size from 120 to 500 -> increased 43% to 57% accuracy 
Removed two layers and had only one dropout layer at 25% -> accuracy: 0.5941
Removed two layers, removed dropout rate, now only look at 7 features, added regularization to second hidden layer -> 0.6277056
added third hidden layer, reduced first layer from 128 to 7 nodes, added regularization to three hidden layers -> 0.5844156
removed regularization from middle hidden layer -> 0.5454546
removed regularization from first hidden layer -> 0.5887446
add regularization to middle layer -> 0.5627705
7 hidden layer, layer 1 128 nodes, layers 2-7 64 nodes, regularization on layers 4 and 7 -> 0.5021645
add bias of 2.0 to first layer -> 0.5454546
3 hidden layers, regularization on last 2, bias of 2.0 on first layer -> 0.4935065
same as above but reg on hidden layers 1 and 3 -> 0.5324675
increase nodes in first layer back up from 7 to 128 -> 0.6190476
move bias from input to output node -> 0.5584416
add alcohol col as a parameter -> 100% (not the good 100 tho lol)
reduce nodes in input layer to 8 from 128 -> 100%
L1 = https://www.analyticssteps.com/blogs/l2-and-l1-regularization-machine-learning

Wow!!! Changed loss to binary_crossentropy -> accuracy: 0.8773
No increase in accuracy by changing epochs to 300
```{r, echo=FALSE}

wineModel = keras_model_sequential() %>%
  layer_dense(units = 32, activation = "relu",input_shape=(10), regularizer_l1(l = 0.01)) %>%
  layer_dense(units = 32, activation = "relu", regularizer_l1(l = 0.01)) %>%
  layer_dense(units = 32, activation = "relu", regularizer_l1(l = 0.01)) %>%
  layer_dense(units = 32, activation = "relu", regularizer_l1(l = 0.01)) %>%
  layer_dense(units = ncol(yTr), activation = "softmax")


wineModel %>% compile(
  loss = "binary_crossentropy",
  optimizer = "adam", #optimizer_rmsprop(),
  metrics = "accuracy"
)

xTr <- as.matrix(training[,1:10]) # need to convert to a matrix
xTest <- as.matrix(testing[,1:10])

history <- wineModel %>% 
  fit(
    x = xTr, # input is the first 4 columns of the dataframe
    y = yTr, # label is the last column
    epochs = 50
  )

```
```{r}
plot(history)
summary(wineModel)
```

# Evaluate the model
```{r}
wineModel %>% evaluate(xTest, yTest)

# Predicting likelihood of all categories:
result <- wineModel %>% predict(xTest)

result

testing[,11]

```
